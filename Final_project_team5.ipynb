{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kangsunwoo827/rl_23_project_team5/blob/main/Final_project_team5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbfyMHoTJckJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "from gym import spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSKsPWTZKc3w"
      },
      "outputs": [],
      "source": [
        "##########윷놀이 환경 설정#############\n",
        "# 출발 전 말은 0, 나간 말은 30번째로.\n",
        "#\n",
        "#   11 10  9  8  7  6\n",
        "#   12 26       21  5\n",
        "#   13    27  22    4\n",
        "#           23\n",
        "#   14   24   28    3\n",
        "#   15 25        29 2\n",
        "#   16 17 18 19 20  1\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vU-dT56nwa0"
      },
      "outputs": [],
      "source": [
        "empty_board='''\n",
        " -----------------------\n",
        "| {}  {}  {}  {}  {}  {}|\n",
        "|                       |\n",
        "| {}   {}        {}   {}|\n",
        "|        {}    {}       |\n",
        "| {}                  {}|\n",
        "|           {}          |\n",
        "| {}                  {}|\n",
        "|        {}    {}       |\n",
        "| {}   {}        {}   {}|\n",
        "|                       |\n",
        "| {}  {}  {}  {}  {}  {}|\n",
        " -----------------------\n",
        " '''\n",
        "\n",
        "board2num={\n",
        "    11:0,10:1,9:2,8:3,7:4,6:5,\n",
        "    12:6,26:7,       21:8,5:9,\n",
        "          27:10,   22:11,\n",
        "    13:12,               4:13,\n",
        "              23:14,\n",
        "    14:15,               3:16,\n",
        "            24:17,28:18,\n",
        "    15:19, 25:20,  29:21,2:22,\n",
        "    16:23,17:24,18:25,19:26,20:27,1:28\n",
        " }\n",
        "\n",
        "num2yut={\n",
        "    1:'도',2:'개',3:'걸',4:'윷',5:'모',-1:'빽도'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47tu7nG-JweN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10641bd-4b32-4795-e62a-0a38bbaba3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class YutEnv(gym.Env): # gym.Env를 상속\n",
        "    def __init__(self,silent):\n",
        "      self.silent=silent\n",
        "      #뒤집어질 확률\n",
        "      self.p=0.6\n",
        "      self.p_lst=[\n",
        "          3*(self.p**1)*((1-self.p)**3),\n",
        "          6*(self.p**2)*((1-self.p)**2),\n",
        "          4*(self.p**3)*((1-self.p)**1),\n",
        "          1*(self.p**4)*((1-self.p)**0),\n",
        "          1*(self.p**0)*((1-self.p)**4),\n",
        "          1*(self.p**1)*((1-self.p)**3),\n",
        "          ]\n",
        "      #도 개 걸 윷 모 빽도\n",
        "\n",
        "      \n",
        "      self.pos= np.zeros(8,dtype=int)\n",
        "      self._make_board()\n",
        "      self.done=False\n",
        "      self.team=np.random.choice([1,-1])\n",
        "      self.bonus=0\n",
        "      self.turn_num=0\n",
        "\n",
        "      self.high=np.ones(249)+np.eye(249)[-1]*4\n",
        "      self.action_space = spaces.Discrete(4)\n",
        "      self.observation_space = spaces.Box(low=np.zeros(249),high=self.high,dtype=int)\n",
        "      \n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self.turn_num+=1\n",
        "        if not self.silent:\n",
        "          print('{}번째 차례!'.format(self.turn_num))\n",
        "          print('팀 {}의 차례입니다.'.format(self._num2team(self.team)))\n",
        "          print('두구두구두구.... {}!!'.format(num2yut[self.move_num]))\n",
        "        #random policy\n",
        "        if not 0<=action<=3:\n",
        "          raise Exception('action이 잘못됐습니다. cur_action is {}'.format(action))\n",
        "\n",
        "        \n",
        "        prev_pos=self.pos[action]\n",
        "        next_pos=self._move(self.pos[action],self.move_num)\n",
        "\n",
        "        if not self.silent:\n",
        "          print('{}번째 말을 옮깁니다.'.format(action))\n",
        "          print('이동 후 pos : ',self.pos)\n",
        "        #윷, 모인 경우 한번 더!\n",
        "        if self.move_num>=4:\n",
        "          self.bonus=1\n",
        "          \n",
        "        for i in range(4):\n",
        "          #업은 경우\n",
        "          # i=i*self.team\n",
        "          if prev_pos>0 and self.pos[i]==prev_pos:\n",
        "            self.pos[i]=next_pos\n",
        "\n",
        "          #잡은경우\n",
        "          if 0<next_pos<30 and self.pos[-(i+1)]==next_pos:\n",
        "            if not self.silent:\n",
        "              print('잡았습니다!')\n",
        "            self.pos[-(i+1)]=0\n",
        "            self.bonus=1\n",
        "\n",
        "        self.pos[action]=next_pos\n",
        "\n",
        "\n",
        "        self._check_end()\n",
        "\n",
        "        reward = self.bonus #정의해야함.\n",
        "\n",
        "        #팀 바꾸기\n",
        "        if self.bonus:\n",
        "          self.bonus=0\n",
        "          if not self.silent:\n",
        "            print('한번 더!')\n",
        "        else:\n",
        "          self.pos=self.pos[::-1]\n",
        "          self.team*=-1\n",
        "\n",
        "        \n",
        "        self._make_board()\n",
        "\n",
        "        obs=self._get_state()\n",
        "        \n",
        "\n",
        "        return obs, reward, done, None\n",
        "\n",
        "      \n",
        "    def _check_end(self):\n",
        "      if sum(self.pos[:4])==120 or sum(self.pos[-4:])==120:\n",
        "        self.done=True\n",
        "\n",
        "    def _move(self, prev_pos, move_num):\n",
        "      #일단 빽도는 고려하지 않음.\n",
        "      for i in range(move_num):\n",
        "        next_pos=self._move_one(prev_pos,short_cut=bool(i==0))\n",
        "        prev_pos=next_pos\n",
        "      return next_pos\n",
        "\n",
        "    def _move_one(self,prev_pos,short_cut=False):\n",
        "      if short_cut:\n",
        "        if prev_pos==6:\n",
        "          return 21\n",
        "        elif prev_pos==11:\n",
        "          return 26\n",
        "        elif prev_pos==23:\n",
        "          return 28\n",
        "      if prev_pos in [1,30]:\n",
        "        return 30\n",
        "      elif prev_pos==0:\n",
        "        return 2\n",
        "      elif prev_pos==29:\n",
        "        return 1\n",
        "      else:\n",
        "        return prev_pos+1\n",
        "\n",
        "    def reset(self):\n",
        "      self.pos= np.zeros(8,dtype=int)\n",
        "      self._make_board()\n",
        "      self.done=False\n",
        "      self.team=np.random.choice([1,-1])\n",
        "      self.bonus=0\n",
        "      self.turn_num=0\n",
        "      return self._get_state()\n",
        "\n",
        "\n",
        "    def _make_board(self):\n",
        "      # 앞에 4개는 우리팀 말들의 위치, 뒤 4개는 상대팀 말들의 위치\n",
        "      self.board = np.array([np.eye(31, dtype=int)[self.pos[i]] for i in range(8)])\n",
        "      \n",
        "      return self.board\n",
        "      \n",
        "    def _throw_yut(self):\n",
        "      #yut_num은 던졌을 때 뒤집어진 개수. 0이면 모, 4면 윷\n",
        "      self.move_num=np.random.choice([1,2,3,4,5,-1], p=self.p_lst)\n",
        "\n",
        "      #빽도 없애는 코드\n",
        "      if self.move_num==-1:\n",
        "        self.move_num=1\n",
        "      \n",
        "      return self.move_num\n",
        "\n",
        "    def _get_state(self):\n",
        "      #현재 보드 판 상태와 내가 움직여야 하는 개수 return\n",
        "      return np.append(self.board.flatten(),self._throw_yut())\n",
        "\n",
        "    def _num2team(self,team):\n",
        "      return \"★\" if team>0 else '■'\n",
        "    \n",
        "    def render(self):\n",
        "      #내 말은 ● 상대 말은 X로 표시\n",
        "      base=['◎']+['○']*4+['◎']+['○']*8+['◎']+['○']*8+['◎']+['○']*4+['◎']\n",
        "      \n",
        "      for i in range(4):\n",
        "        if 0<self.pos[i]<30:\n",
        "          base[board2num[self.pos[i]]]=self._num2team(self.team)\n",
        "        if 0<self.pos[-(i+1)]<30:\n",
        "          base[board2num[self.pos[-(i+1)]]]=self._num2team(-self.team)\n",
        "\n",
        "      if not self.silent:\n",
        "        print(empty_board.format(*base))\n",
        "      return empty_board.format(*base)\n",
        "\n",
        "    def show_result(self):\n",
        "      if not self.silent:\n",
        "        print(\"team{} is win!\".format(self._num2team(self.team)))\n",
        "      return self.team\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op-ieHQanAaj",
        "outputId": "035b17d4-926e-4357-b20c-7f42484c1aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==\n"
          ]
        }
      ],
      "source": [
        "def random_policy(state):\n",
        "  possible_action=[]\n",
        "  for i in range(4):\n",
        "\n",
        "    if not state[31*(i+1)-1]==1: #도착한 말이 아니라면\n",
        "      possible_action.append(i)\n",
        "  return np.random.choice(possible_action)\n",
        "\n",
        "env=YutEnv(silent=True)\n",
        "env.render()\n",
        "print('==')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2VarJnbkayJ"
      },
      "outputs": [],
      "source": [
        "# res=[]\n",
        "# for iter in range(1000):\n",
        "#   done=False\n",
        "#   state=env.reset()\n",
        "#   while not env.done:\n",
        "#     action=random_policy(state)\n",
        "#     state,return_value, done = env.step(action)\n",
        "\n",
        "#   winner=env.show_result()\n",
        "#   res.append(winner)\n",
        "\n",
        "# print(res.count(1))\n",
        "# print(res.count(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu23FLNBbCyN"
      },
      "outputs": [],
      "source": [
        "# def play(show_number):\n",
        "#     env = YutEnv(show_number=show_number)\n",
        "#     agents = [HumanAgent('O'),\n",
        "#               HumanAgent('X')]\n",
        "#     episode = 0\n",
        "#     while True:\n",
        "#         state = env.reset()\n",
        "#         _, mark = state\n",
        "#         done = False\n",
        "#         env.render()\n",
        "#         while not done:\n",
        "#             agent = agent_by_mark(agents, next_mark(mark))\n",
        "#             env.show_turn(True, mark)\n",
        "#             ava_actions = env.available_actions()\n",
        "#             action = agent.act(ava_actions)\n",
        "#             if action is None:\n",
        "#                 sys.exit()\n",
        "\n",
        "#             state, reward, done, info = env.step(action)\n",
        "\n",
        "#             print('')\n",
        "#             env.render()\n",
        "#             if done:\n",
        "#                 env.show_result(True, mark, reward)\n",
        "#                 break\n",
        "#             else:\n",
        "#                 _, mark = state\n",
        "#         episode += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48osWymZh2hQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F4Kh_YoGHRY"
      },
      "outputs": [],
      "source": [
        "def policy_by_team(policies,team):\n",
        "  return policies[0] if team==1 else policies[1]\n",
        "policies=[random_policy,random_policy]\n",
        "\n",
        "def play(iteration_num,policies=[random_policy,random_policy]):\n",
        "  res=[]\n",
        "  for iter in range(iteration_num):\n",
        "    done=False\n",
        "    state=env.reset()\n",
        "    while not env.done:\n",
        "      cur_policy=policy_by_team(policies,env.team)\n",
        "      action=cur_policy(state)\n",
        "      state,return_value, done = env.step(action)\n",
        "\n",
        "    winner=env.show_result()\n",
        "    res.append(winner)\n",
        "\n",
        "  print('team ★ win :', res.count(1))\n",
        "  print('team ■ win :', res.count(-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# play(100)"
      ],
      "metadata": {
        "id": "QlskHKzLGeGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DQN START**"
      ],
      "metadata": {
        "id": "_davtJx4HKj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define neural net Q_\\theta(s,a) as a class\n",
        "\n",
        "class Qfunction(keras.Model):\n",
        "    \n",
        "    def __init__(self, obssize, actsize, hidden_dims):\n",
        "        \"\"\"\n",
        "        obssize: dimension of state space\n",
        "        actsize: dimension of action space\n",
        "        hidden_dims: list containing output dimension of hidden layers \n",
        "        \"\"\"\n",
        "        super(Qfunction, self).__init__()\n",
        "\n",
        "        # Layer weight initializer\n",
        "        initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.)\n",
        "\n",
        "        # Input Layer\n",
        "        self.input_layer = keras.layers.InputLayer(input_shape=(obssize,))\n",
        "        \n",
        "        # Hidden Layer\n",
        "        self.hidden_layers = []\n",
        "        for hidden_dim in hidden_dims:\n",
        "            # TODO: define each hidden layers\n",
        "            layer = keras.layers.Dense(hidden_dim, activation='relu',\n",
        "                                      kernel_initializer=initializer)\n",
        "            self.hidden_layers.append(layer) \n",
        "        \n",
        "        # Output Layer : \n",
        "        # TODO: Define the output layer.\n",
        "        self.output_layer = keras.layers.Dense(actsize) \n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, states):\n",
        "        # Connect the layers and pass the input tensor through them\n",
        "        x = self.input_layer(states)\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "        output = self.output_layer(x)\n",
        "        \n",
        "        return output\n"
      ],
      "metadata": {
        "id": "FD-2UYqvG5ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper class for training Qfunction and updating weights (target network) \n",
        "\n",
        "class DQN(object):\n",
        "    \n",
        "    def __init__(self, obssize, actsize, hidden_dims, optimizer):\n",
        "        \"\"\"\n",
        "        obssize: dimension of state space\n",
        "        actsize: dimension of action space\n",
        "        optimizer: \n",
        "        \"\"\"\n",
        "        self.qfunction = Qfunction(obssize, actsize, hidden_dims)\n",
        "        self.optimizer = optimizer\n",
        "        self.obssize = obssize\n",
        "        self.actsize = actsize\n",
        "\n",
        "    def _predict_q(self, states, actions):\n",
        "        \"\"\"\n",
        "        states represent s_t\n",
        "        actions represent a_t\n",
        "        \"\"\"\n",
        "        q_values = self.qfunction(states)\n",
        "\n",
        "        action_indices = tf.stack([tf.range(tf.shape(actions)[0]), actions], axis=-1)\n",
        "        q_values = tf.gather_nd(q_values, action_indices)\n",
        "        \n",
        "        return q_values\n",
        "\n",
        "    def _loss(self, Qpreds, targets):\n",
        "        \"\"\"\n",
        "        Qpreds represent Q_\\theta(s,a)\n",
        "        targets represent the terms E[r+gamma Q] in Bellman equations\n",
        "\n",
        "        This function is OBJECTIVE function\n",
        "        \"\"\"\n",
        "        return tf.math.reduce_mean(tf.square(Qpreds - targets))\n",
        "\n",
        "    \n",
        "    def compute_Qvalues(self, states):\n",
        "        \"\"\"\n",
        "        states: numpy array as input to the neural net, states should have\n",
        "        size [numsamples, obssize], where numsamples is the number of samples\n",
        "        output: Q values for these states. The output should have size \n",
        "        [numsamples, actsize] as numpy array\n",
        "        \"\"\"\n",
        "        inputs = np.atleast_2d(states.astype('float32'))\n",
        "        return self.qfunction(inputs)\n",
        "\n",
        "\n",
        "    def train(self, states, actions, targets):\n",
        "        \"\"\"\n",
        "        states: numpy array as input to compute loss (s)\n",
        "        actions: numpy array as input to compute loss (a)\n",
        "        targets: numpy array as input to compute loss (Q targets)\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            Qpreds = self._predict_q(states, actions)\n",
        "            loss = self._loss(Qpreds, targets)\n",
        "        variables = self.qfunction.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        return loss\n",
        "\n",
        "    def update_weights(self, from_network):\n",
        "        \"\"\"\n",
        "        We need a subroutine to update target network \n",
        "        i.e. to copy from principal network to target network. \n",
        "        This function is for copying  𝜃←𝜃target \n",
        "        \"\"\"\n",
        "        \n",
        "        from_var = from_network.qfunction.trainable_variables\n",
        "        to_var = self.qfunction.trainable_variables\n",
        "        \n",
        "        for v1, v2 in zip(from_var, to_var):\n",
        "            v2.assign(v1)"
      ],
      "metadata": {
        "id": "qhDmUbs3HR5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement replay buffer\n",
        "class ReplayBuffer(object):\n",
        "    \n",
        "    def __init__(self, maxlength):\n",
        "        \"\"\"\n",
        "        maxlength: max number of tuples to store in the buffer\n",
        "        if there are more tuples than maxlength, pop out the oldest tuples\n",
        "        \"\"\"\n",
        "        self.buffer = deque()\n",
        "        self.number = 0\n",
        "        self.maxlength = maxlength\n",
        "    \n",
        "    def append(self, experience):\n",
        "        \"\"\"\n",
        "        this function implements appending new experience tuple\n",
        "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "        self.number += 1\n",
        "        if(self.number > self.maxlength):\n",
        "            self.pop()\n",
        "        \n",
        "    def pop(self):\n",
        "        \"\"\"\n",
        "        pop out the oldest tuples if self.number > self.maxlength\n",
        "        \"\"\"\n",
        "        while self.number > self.maxlength:\n",
        "            self.buffer.popleft()\n",
        "            self.number -= 1\n",
        "    \n",
        "    def sample(self, batchsize):\n",
        "        \"\"\"\n",
        "        this function samples 'batchsize' experience tuples\n",
        "        batchsize: size of the minibatch to be sampled\n",
        "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
        "        \"\"\"\n",
        "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
        "        return [self.buffer[idx] for idx in inds]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWxBngUZHYYL",
        "outputId": "146c6998-f1c7-4c7d-e7d1-2a97194291a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:14: DeprecationWarning: invalid escape sequence '\\p'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO: Set the required parameters. All parameters can be tuned by yourself.\n",
        "lr = 5e-4  # learning rate for gradient update \n",
        "batchsize = 64  # batchsize for buffer sampling\n",
        "maxlength = 10000  # max number of tuples held by buffer\n",
        "tau = 100  # time steps for target update\n",
        "episodes = 3000  # number of episodes to run\n",
        "initialize = 500  # initial time steps before start updating\n",
        "epsilon = 1  # constant for exploration\n",
        "gamma = 0.99  # discount\n",
        "hidden_dims = [24,24] # hidden dimensions\n",
        "epsilon_decay=0.95\n",
        "min_epsilon=0.005\n",
        "################################################################################\n",
        "\n",
        "# initialize environment\n",
        "env = YutEnv(silent=True)\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n\n",
        "\n",
        "# optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# initialize networks\n",
        "Qprincipal = DQN(obssize, actsize, hidden_dims, optimizer)\n",
        "Qtarget = DQN(obssize, actsize, hidden_dims, optimizer)\n",
        "\n",
        "# initialization of buffer\n",
        "buffer = ReplayBuffer(maxlength)\n",
        "\n",
        "################################################################################\n",
        "# TODO: Complete the main iteration"
      ],
      "metadata": {
        "id": "LBVhoAYnNbPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sSPrUqeQ_eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TODO: Complete the main iteration\n",
        "# CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials.\n",
        "\n",
        "rrecord = []\n",
        "totalstep = 0\n",
        "for ite in range(episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    rsum = 0\n",
        "    time=0\n",
        "    if epsilon > min_epsilon:\n",
        "      epsilon *= epsilon_decay\n",
        "\n",
        "    while not done:\n",
        "        # Choose action\n",
        "        #별만 학습\n",
        "        if env.team==1:\n",
        "          if np.random.uniform() < epsilon:\n",
        "              # action = env.action_space.sample()\n",
        "              action = random_policy(obs)\n",
        "          else:\n",
        "              q_values = Qprincipal.compute_Qvalues(obs)\n",
        "              action = np.argmax(q_values)\n",
        "        else:\n",
        "          action = random_policy(obs)\n",
        "\n",
        "        next, reward, done, _ = env.step(action)\n",
        "\n",
        "\n",
        "        if env.team==1\n",
        "          buffer.append((obs, action, reward, next))\n",
        "\n",
        "          obs = next\n",
        "          rsum += reward\n",
        "          time += 1\n",
        "          totalstep += 1\n",
        "          \n",
        "          if totalstep > initialize:\n",
        "              batch = buffer.sample(batchsize)\n",
        "              states = np.array([b[0] for b in batch])\n",
        "              actions = np.array([b[1] for b in batch])\n",
        "              rewards = np.array([b[2] for b in batch])\n",
        "              next_states = np.array([b[3] for b in batch])\n",
        "\n",
        "              next_qvalues = Qtarget.compute_Qvalues(next_states)\n",
        "              targets = rewards + gamma * np.max(next_qvalues, axis=1)\n",
        "\n",
        "              Qprincipal.train(states, actions, targets)\n",
        "              if totalstep % tau == 0:\n",
        "                  Qtarget.update_weights(Qprincipal)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "    ## DO NOT CHANGE THIS PART! \n",
        "    rrecord.append(rsum)\n",
        "    if ite % 10 == 0:\n",
        "        print('iteration {} average reward {}'.format(ite, np.mean(rrecord[-10:])))\n",
        "    \n",
        "    ave100 = np.mean(rrecord[-100:])   \n",
        "    if  ave100 > 195.0:\n",
        "        print(\"Solved after %d episodes.\"%ite)\n",
        "        break"
      ],
      "metadata": {
        "id": "JoPPhLQdHqy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}